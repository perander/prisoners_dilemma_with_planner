{
    "ppo": {
        "epochs": 2,
        "episodes": 20000,
        "n_steps": 300,
        "batch_size":64,
        "lr": 0.0002,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "policy_clip": 0.2,
        "ent_coef": 0.2,
        "vf_coef": 0.5,
        "t_learning_starts": 0,
        "training_frequency": 256,
        "anneal_lr": false
    },
    "dqn": {
        "episodes": 20000,
        "batch_size": 32,
        "buffer_size": 20000,
        "lr": 0.0005,
        "gamma": 0.99,
        "tau": 0.005,
        "start_e": 1,
        "end_e": 0.05,
        "exploration_fraction": 0.001,
        "t_learning_starts": 1000,
        "training_frequency": 4
    },
    "ppo_planner": {
        "epochs": 2,
        "episodes": 20000,
        "n_steps": 300,
        "batch_size": 64,
        "lr": 0.0003,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "policy_clip": 0.2,
        "ent_coef": 0.1,
        "vf_coef": 0.5,
        "t_learning_starts": 0,
        "training_frequency": 256,
        "n_actions_per_agent": 5,
        "max_reward": 2,
        "anneal_lr": false
    },
    "q_planner": {
        "states": 4,
        "actions": 3,
        "lr": 0.1,
        "gamma": 0.999999,
        "epsilon": 0.999,
        "min_epsilon": 0.1,
        "n_actions_per_agent": 5,
        "max_reward": 2,
        "n_states": 4,
        "t_learning_starts": 0,
        "training_frequency": 1
    },
    "dummy": {
        "training_frequency": 1,
        "t_learning_starts": 0
    }
}